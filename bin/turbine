#!/usr/bin/env python
from __future__ import print_function, absolute_import, division
import math
from os.path import split as path_split
from collections import namedtuple
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

try:
    import sklearn
    sklearn_version = map(int, sklearn.__version__.split('.'))
    if sklearn_version[0] == 0 and sklearn_version[1] < 18:
        exit('Too old sklearn, needs at least 0.18.2, was: %s' %
             sklearn.__version__)
except ImportError as err:
    exit('Failed to import sklearn, have you install scikit learn?\n%s\n' % err)

try:
    from turbinelearn import *
except ImportError as err:
    exit('Failed to import turbinelearn\n%s' % err)


LearningParameter = namedtuple('LearningParameter',
                               ['degree', 'training_fraction', 'k',
                                'dataset', 'clean', 'normalize',
                                'dual_model'])


def visualize(data, training_data, test_data, reg_mod):
    data_timeline = data[1].index
    training_data_timeline = training_data[1].index
    test_data_timeline = test_data[1].index

    plt.plot(
        data_timeline, data[1], "ro",
        training_data_timeline, reg_mod.predict(training_data[0]), "bo",
        test_data_timeline, reg_mod.predict(test_data[0]), "go",
        markersize=2)


def create_argparse():
    parser = argparse.ArgumentParser(description='Learns a polynomial.')
    parser.add_argument('-m', '--method', dest='method', choices=['fcv', 'icv', 'simple', 'pca', 'reg'], required=True,
                        help='Uses either filebased cross validation (fcv), individual file cross validation (icv), run a single train-test run on each file (simple) or do PCA (pca)')
    parser.add_argument('-d', '--degree', dest='degree', type=int, choices=range(1, 10), default=2,
                        help='The degree of the polynomial to train, defaults to 2')

    parser.add_argument('--clean', dest='clean', action='store_true',
                        help='Whether to perform data cleanup (recommended), defaults to --clean.')
    parser.add_argument('--no-clean', dest='clean', action='store_false')
    parser.set_defaults(clean=True)
    parser.add_argument('--normalize', dest='normalize', action='store_true',
                        help='Whether to perform data normalization (not needed for polynomial model), defaults to --no-normalize.')
    parser.add_argument('--no-normalize', dest='normalize', action='store_false')
    parser.set_defaults(normalize=False)

    parser.add_argument('-t', '--training-fraction', dest='training_fraction', type=float, default=0.6,
                        help='The fraction of data to use as training data, only used when method=simple')
    parser.add_argument('-k', '--k', dest='k', type=int, choices=range(1, 10), default=2,
                        help='The number of folds to keep out when using cross validation. For method=fcv it is the number of files to keep out')
    parser.add_argument('--dataset', dest='dataset', choices=['HG', 'HT', 'all'], default='all',
                        help='The dataset to use, HG, HT, or all.')

    parser.add_argument('--dual', dest='dual', action='store_true',
                        help='Will build separate models for HG and HT before combining them, defaults to --no-dual')
    parser.add_argument('--no-dual', dest='dual', action='store_false')
    parser.set_defaults(dual=False)

    parser.add_argument('-v', '--version', dest='version', action='store_true',
                        help='Prints version and exits.')

    return parser.parse_known_args()


def get_limits(params):
    return LIMITS if params.clean else {}

def get_normalize(params):
    return FEATURES if params.normalize else ()


def method_fcv(params):
    test_data = file_cross_val(params.dataset,
                               k=params.k,
                               degree=params.degree,
                               limits=get_limits(params),
                               normalize=get_normalize(params),
                               dual_model=params.dual_model)

    print("\n\n\n\n################ SUMMARY ###############")
    print(" Cross validations performed: %d" % len(test_data))

    test_data.sort(key=lambda result: result[2][0])
    training_scores = zip(*(zip(*test_data)[2]))[0]

    print("\n R^2 training score:")
    print("\t- Average: \t%.6f" % np.average(training_scores))
    print("\t- Median:  \t%.6f" % np.median(training_scores))
    print("\t- Std dev: \t%.6f" % np.std(training_scores))
    print("\t- Worst:   \t%.6f  (obtained when testing on: [%s])" %
                                (training_scores[0], ", ".join(test_data[0][1])))

    test_data.sort(key=lambda result: min(result[2][1:]))
    test_scores = [result[1:] for result in zip(*test_data)[2]]
    all_test_scores = reduce(lambda x,y: x+y, test_scores)

    print("\n R^2 test score:")
    print("\t- Average: \t%.6f" % np.average(all_test_scores))
    print("\t- Median:  \t%.6f" % np.median(all_test_scores))
    print("\t- Std dev: \t%.6f" % np.std(all_test_scores))
    print("\t- Worst:   \t%r  (obtained when testing on: [%s])" %
                              (test_scores[0], ", ".join(test_data[0][1])))

    print("\n########################################")



def method_icv(params):
    for input_file in params.dataset:
        print("\nDoing individual cross validation for %s" % input_file)
        individual_cross_validation(input_file,
                                    k=params.k,
                                    degree=params.degree,
                                    limits=get_limits(params),
                                    normalize=get_normalize(params))


def method_simple(params):
    for input_file in params.dataset:
        print("\nDoing regression for %s" % input_file)
        full_model= train_and_evaluate_single_file(input_file,
                                                   training_fraction=params.training_fraction,
                                                   degree=params.degree,
                                                   limits=get_limits(params),
                                                   normalize=get_normalize(params))
        visualize(*full_model)


def method_pca(params):
    plt.title('Turbine polynomial PCA')
    i = 0
    N = len(params.dataset)
    for input_file in params.dataset:
        fname = path_split(input_file)
        i = i + 1
        if N > 1:
            plt.subplot(math.ceil(N/2.0), 2, i)
        plt.ylabel(fname[1])
        X_1, X_2, y = pca(input_file,
                          limits=get_limits(params),
                          normalize=get_normalize(params))
        ### Plot the 2 dimensions with y as color of circle
        cmap = sns.cubehelix_palette(as_cmap=True)
        plt.scatter(X_1, X_2, c=y, cmap=cmap)
    plt.show()


def _plot_reg(params, data, i):
    plt.subplot(len(params.dataset), 2, (2*i)+2)
    plt.ylim([0, 20*1000])
    p_speed, = plt.plot(data['TIME'],
                        data['SPEED'],
                        'o', markersize=2, label='SPEED')
    p_dtemp, = plt.plot(data['TIME'],
                        data['DISCHARGE_TEMP'] * 10,
                        'o', markersize=2, label='DISCHARGE_TEMP')
    p_dpres, = plt.plot(data['TIME'],
                        data['DISCHARGE_PRES'] * 1000,
                        'o', markersize=2, label='DISCHARGE_PRES')
    plt.legend(handles=[p_speed, p_dtemp, p_dpres])


def method_reg(params):
    plt.title('Turbine polynomial')
    N = len(params.dataset)
    for i in range(N):
        input_file = params.dataset[i]
        fname = path_split(input_file)
        plt.subplot(len(params.dataset), 2, (2*i)+1)
        plt.ylabel(fname[1])
        print("\nDoing regression for %s" % input_file)
        data, full_model = regression(input_file,
                                      training_fraction=params.training_fraction,
                                      degree=params.degree,
                                      limits=get_limits(params),
                                      normalize=get_normalize(params))
        visualize(*full_model)
        _plot_reg(params, data, i)
    plt.show()


def main(args, dataset):
    params = LearningParameter(degree=args.degree,
                               k=args.k,
                               training_fraction=args.training_fraction,
                               dataset=dataset,
                               clean=args.clean,
                               normalize=args.normalize,
                               dual_model=args.dual)
    print_params(args, dataset)
    methods = {
        'fcv'    : method_fcv,
        'icv'    : method_icv,
        'simple' : method_simple,
        'pca'    : method_pca,
        'reg'    : method_reg
    }
    fn = methods[args.method]
    fn(params)


def print_params(args, dataset):
    print('\nSettings:')
    print('\tDataset:     %s' % dataset)
    print('\tMethod:      %s' % args.method)
    print('\tdual:        %s' % ('Dual model' if args.dual else 'Monomodel'))
    print('\tdegree:      %s' % args.degree)
    print('\tk-fold cv:   k=%s' % args.k)
    print('\ttraining_q:  %s' % args.training_fraction)
    print('\tcleaning:    %s' % ('Cleaning data' if args.clean else 'No cleaning'))
    print('\tnormalizing: %s' % ('Normalizing data' if args.normalize else 'No normalizing'))
    print('-------------------\n\n')


def filter_dataset(dataset, filter):
    """filter in (None, 'HG', HT')"""
    if filter not in (None, '', False, 'all', 'HG', 'HT'):
        raise KeyError('Unknown filter "%s"' % filter)
    if not filter or filter == 'all':
        return dataset
    return [fname for fname in dataset if filter in fname]

if __name__ == "__main__":
    args, dataset = create_argparse()
    if args.dual and args.dataset in ['HG', 'HT']:
        exit('Cannot use both --dual and --dataset')
    if args.version:
        print('turbinelearn version %s' % TURBINELEARN_VERSION)
        exit(0)
    dataset = filter_dataset(dataset, args.dataset)
    if not dataset:
        exit('Usage: turbine --method=pca data/LOCO*csv')
    main(args, dataset)
